---
title: "Data selection of Black Sea Turbot using the SAM assessment model"
author: "Niels Hintzen & Thomas Brunel"
date:   "24th of July 2019"
output: html_document
---

```{r, eval=T, results="hide", echo=TRUE, message=FALSE, warning=FALSE}
library(FLCore)
library(FLSAM) #note that FLSAM relies on install_github("fishfollower/SAM",ref="components")
library(FLEDA)
options(width = 140)
rm(list=ls())
setwd("D:/Repository/MSE_Adriatic/BST/")
source("BST_additionalPlottingRoutines.r")
```

# Intro
In this document we describe the analyses of the Black Sea Turbot assessment data
and describe how we work towards an ultimate model selection. We start with reading
in the relevant data. These data are available from the sharepoint site.

```{r, eval=T, echo=TRUE, message=FALSE, warning=FALSE}
load(file="Turbot_stk_July2019.RData")
load(file="Turbot_idx_July2019.RData")
BST                   <- stk
BST.tun               <- idx
```

There usually are some additional settings to set right, so we continue with that.

```{r, eval=T, echo=TRUE, message=FALSE, warning=FALSE}
BST@m[]               <- c(0.343,0.280,0.242,0.217,0.200,0.187,0.177,0.169,0.163,0.158)
BST@catch             <- BST@landings
units(BST)[1:17]      <- as.list(c(rep(c("tonnes","thousands","kg"),4),
                                 rep("NA",2),"f",rep("NA",2)))
BST@discards.n[]      <- 0; BST@discards.wt[] <- 0; BST@discards <- computeDiscards(BST)
BST@range["minfbar"]  <- 4
BST@range["maxfbar"]  <- 8
BST@name              <- "Black Sea Turbot"

summary(BST)
summary(BST.tun)
```

Now that we have the data ready, we have a look at some of the catch and survey data.
First we have a look at the most important catch data.

```{r, eval=T, echo=FALSE, message=FALSE, warning=FALSE}
print(stacked.area.plot(data~year| unit, as.data.frame(pay(BST@catch.n)),groups="age",main="Proportion of catch numbers at age",ylim=c(-0.01,1.01),xlab="years",col=gray(9:0/9)))
```

First of all, before 1989, there are no age 1 data (there seem to be a lot of dummy values entered!!), so for a model including a longer time-series we
may want to start at the age of 2.
Furthermore, this shows that somewhere end 1980s a substantial change in the fishery occurred, followed by a much larger portion of young fish to be caught.
Near the end of the time-series, the catch composition becomes more similar to the early period. All in all, the proportion of old fish
is rather large throughout the entire time-series. This is important to note as often the plusgroup simply contains too few fish to estimate
a selection for. This seems not to be the case here.

Next up is the survey data. We first look at the internal consistency in these datasets
```{r, eval=T, echo=FALSE, message=FALSE, warning=FALSE}
plot(BST.tun[[1]],type="internal",main=names(BST.tun)[1])
plot(BST.tun[[2]],main=names(BST.tun)[2])
plot(BST.tun[[3]],type="internal",main=names(BST.tun)[3])
plot(BST.tun[[4]],main=names(BST.tun)[4])
plot(BST.tun[[5]],type="internal",main=names(BST.tun)[5])
plot(BST.tun[[6]],type="internal",main=names(BST.tun)[6])
plot(BST.tun[[7]],main=names(BST.tun)[7])
plot(BST.tun[[8]],type="internal",main=names(BST.tun)[8])
```

Overall, the internal consistency looks not too good. Except for the Ukrain and Bulgarian surveys that seem to be able to track cohorts a little better.

We can make a similar plot for the catch data, to see how good cohort tracking is there.
```{r, eval=T, echo=FALSE, message=FALSE, warning=FALSE}
catchAsTun <- FLIndices(catchAsTun=FLIndex(index=BST@catch.n))
plot(catchAsTun[[1]],type="internal",main="Entire time-series")
catchAsTun <- FLIndices(catchAsTun=FLIndex(index=BST@catch.n[,ac(1989:2018)]))
plot(catchAsTun[[1]],type="internal",main="1989-2018")
```

This doesn't look too good either. Even the catch-at-age data has very low internal consistency at ages 2-4 and 8-10.
Reasons could be related to ageing error, or different countries (in different regions) adding to the catch-at-age matrix in different ratios over time.

# Data observation based data selection

The group agreed to continue with the full time-series length for the assessment
All surveys fit within this range, so we don't need to truncate those in time.
However, some of the surveys span to age 10. All ages in the surveys are true ages, and therefore we need to
truncate the age 10 data as in the assessment model, age 10 is a plusgroup and hence won't match up.
Next to that, because there is no age 1 data in the early time-period, we have to truncate that age as well and start at the age of 2.

```{r, eval=T, echo=FALSE, message=FALSE, warning=FALSE}
BST.tun                 <- lapply(BST.tun,function(x){x@range["plusgroup"] <- NA; return(x)})
BST.tun[["RO_Spring"]]  <- trim(BST.tun[["RO_Spring"]],age=2:6)
BST.tun[["RO_Autumn"]]  <- trim(BST.tun[["RO_Autumn"]],age=2:6)
BST.tun[["UKR_West"]]   <- trim(BST.tun[["UKR_West"]], age=2:9)
BST.tun[["BG_Spring"]]  <- trim(BST.tun[["BG_Spring"]],age=2:9)
BST.tun[["BG_Autumn"]]  <- trim(BST.tun[["BG_Autumn"]],age=2:7)
BST.tun[["TR_Spring"]]  <- trim(BST.tun[["TR_Spring"]],age=2:7)
BST.tun[["UKR_East"]]   <- trim(BST.tun[["UKR_East"]], age=2:9)
BST.tun[["TR_Autumn"]]  <- trim(BST.tun[["TR_Autumn"]],age=2:7)
BST                     <- trim(BST,age=2:10)
```

In some of these survey data, there are dummy values of 0.01 and sometimes 0.02. The SAM model really tries to fit these
so we better get rid of those and replace to NA values instead.

```{r, eval=T, echo=TRUE, message=FALSE, warning=FALSE}
for(iTun in 1:length(BST.tun))
  BST.tun[[iTun]]@index@.Data[which(BST.tun[[iTun]]@index[] <= 0.02)] <- NA
BST@catch.n@.Data[which(BST@catch.n[] <= 0.02)] <- NA
```

Another problem is the large gaps in the survey time-series. For example the Ukrainian surveys lack data between 1995-2000 and the Turkish data between 1997-2009, 2012-2016 and
the Bulgarian between 2013-2015. For every NA value in the SAM assessment, the assessment model tries to estimate a value. In this case, that will result in a very
large number of missing values to be estimated, on top of the data that is not of top-notch quality, this is something to consider.
As such, we drop the gaps and do try to estimate the missing data (which is a feature of SAM but one we don't need here).

```{r, eval=T, echo=TRUE, message=FALSE, warning=FALSE}
BST.tun[["UKR_West"]]       <- BST.tun[["UKR_West"]][,ac(c(1989:1994,1998,2001:2006))]
BST.tun[["UKR_East"]]       <- BST.tun[["UKR_East"]][,ac(c(1989:1994,1998,2001:2006))]
BST.tun[["TR_Autumn"]]      <- BST.tun[["TR_Autumn"]][,ac(c(1991:1996,2010:2011,2017:2018))]
BST.tun[["TR_Spring"]]      <- BST.tun[["TR_Spring"]][,ac(c(1991:1996,2010:2011,2017:2018))]
BST.tun[["BG_Spring"]]      <- BST.tun[["BG_Spring"]][,ac(c(2006:2012,2016:2018))]
BST.tun[["BG_Autumn"]]      <- BST.tun[["BG_Autumn"]][,ac(c(2006:2007,2009:2011,2014:2018))]
BST.tun                     <- lapply(BST.tun,function(x) {x@type <- "number"; return(x)})
save(BST,BST.tun,file="./DSML1.RData")
```

In principle, the most crude data selection is finished now. There are basically two lines one can follow:
1. I keep all the data in that is actually measured. 2. I only keep data in that is actually measured AND informative.
With the SAM model, we do not necessarily need to make a decision between options 1 and 2 because informative data can always
be downweighted by the SAM model itself. It is however informative to have a look.

# Model based data selection

Now that the most obvious problems are out of the way, we continue our data selection based on model performance.
We start off with a default configuration for a SAM assessment in which catchabilities are estimated. We then try to minimize the number
of catchability parameters so that we can free up some other parameters.

```{r, eval=T, echo=TRUE, message=FALSE, warning=FALSE}
BST.ctrl            <- FLSAM.control(BST,BST.tun)
BST.ctrl@residuals  <- FALSE
BST.ctrl            <- update(BST.ctrl)
output              <- capture.output(BST.sam             <- (FLSAM(BST,BST.tun,BST.ctrl)))
save(BST,BST.tun,BST.ctrl,BST.sam,file="./DS2.RData")
```

Note that we can also output a typical SAM object, for e.g. reviewers that have mor affinity with that type of data

```{r, eval=F, echo=TRUE, message=FALSE, warning=FALSE}
BST.fit             <- FLSAM(BST,BST.tun,BST.ctrl,return.fit=T)
BST.sam             <- SAM2FLR(BST.fit,BST.ctrl)

#- or via
data  <- FLSAM2SAM(FLStocks(residual=BST),BST.tun)
conf  <- ctrl2conf(BST.ctrl,data)
par   <- stockassessment::defpar(data,conf)
fit   <- sam.fit(data,conf,par)
```

This gives us the following catchabilities for each of the surveys

```{r, eval=T, echo=FALSE, message=FALSE, warning=FALSE}
catch <- catchabilities(BST.sam)
print(xyplot(value+ubnd+lbnd ~ age | fleet,catch,
             scale=list(alternating=FALSE,y=list(relation="free")),as.table=TRUE,
             type="b",lwd=c(2,1,1),col=c("black","grey","grey"),pch=19,
             subset=fleet %in% names(BST.tun),
             main="Survey catchability parameters_all",ylab="Catchability",xlab="Age"))
```

Based on the confidence intervals, I come up with some sensible binding (not final yet).
Note that I use sequences from 1:x for all, but adding a very large number at the end. When calling the update function
all numbers will be aligned nicely from 0 to as many parameters as we have

```{r, eval=T, echo=TRUE, message=FALSE, warning=FALSE}
BST.ctrl@catchabilities["RO_Spring",ac(2:6)]          <- c(1:2,rep(3,3))
BST.ctrl@catchabilities["RO_Autumn",ac(2:6)]          <- c(1,2,2,3,3)            +101
BST.ctrl@catchabilities["UKR_West", ac(2:9)]          <- c(1,1,2,2,3,4,5,5)      +201
BST.ctrl@catchabilities["BG_Spring",ac(2:9)]          <- c(1,2,2,3,3,3,4,4)      +301
BST.ctrl@catchabilities["BG_Autumn",ac(2:7)]          <- c(1,2,3,4,5,5)          +401
BST.ctrl@catchabilities["TR_Spring",ac(2:7)]          <- c(1,2,3,3,3,3)          +501
BST.ctrl@catchabilities["UKR_East", ac(2:9)]          <- c(1,1,2,2,3,3,4,4)      +601
BST.ctrl@catchabilities["TR_Autumn",ac(2:7)]          <- c(1,2,2,2,3,3)          +701
BST.ctrl                                              <- update(BST.ctrl)
```

Now let's open up some parameters in the observation variances

```{r, eval=T, echo=TRUE, message=FALSE, warning=FALSE}
BST.ctrl@obs.vars["catch unique",]                    <- c(2:9,9)
BST.ctrl@obs.vars["RO_Spring",ac(2:6)]                <- c(2:5,5)               +101
BST.ctrl@obs.vars["RO_Autumn",ac(2:6)]                <- c(2:5,5)               +201
BST.ctrl@obs.vars["UKR_West", ac(2:9)]                <- c(2:8,8)               +301
BST.ctrl@obs.vars["BG_Spring",ac(2:9)]                <- c(2:8,8)               +401
BST.ctrl@obs.vars["BG_Autumn",ac(2:7)]                <- c(2:6,6)               +501
BST.ctrl@obs.vars["TR_Spring",ac(2:7)]                <- c(2:6,6)               +601
BST.ctrl@obs.vars["UKR_East", ac(2:9)]                <- c(2:8,8)               +701
BST.ctrl@obs.vars["TR_Autumn",ac(2:7)]                <- c(2:6,6)               +801
BST.ctrl                                              <- update(BST.ctrl)

```

There is now a lot of freedom in these parameters. We now go and run the model again and look at the output.
We use the previous run as a starting point though, to not make convergence too difficult for ourselves.

```{r, eval=T, echo=TRUE, message=FALSE, warning=FALSE}
output              <- capture.output(BST.sam             <- (FLSAM(BST,BST.tun,BST.ctrl,starting.values=BST.sam)))
save(BST,BST.tun,BST.ctrl,BST.sam,file="./DSML3.RData")
```

Note that convergence is an issue here, so only use these estimate as proxies. Let's look at the parameter values of both
the F random walk variances and the observation variances

```{r, eval=T, echo=FALSE, message=FALSE, warning=FALSE}
obsvar.plot(BST.sam)

obsvars <- obs.var(BST.sam)
print(xyplot(value+ubnd+lbnd ~ age | fleet,obsvars,
             scale=list(alternating=FALSE,y=list(relation="free")),as.table=TRUE,
             type="b",lwd=c(2,1,1),col=c("black","grey","grey"),pch=19,
             main="Observation variances",ylab="Observation variance",xlab="Age"))
```

From the first picture, there are no obvious age-classes that should be omitted as they are simply too noisy. Obvioulsy, those age-classes of surveys in the
right-hand side are candidates, but not directly.
We now go and combine some of the ages so we reduce the number of free parameters


```{r, eval=T, echo=TRUE, message=FALSE, warning=FALSE}
BST.ctrl@obs.vars["catch unique",]                    <- c(2:9,9)
BST.ctrl@obs.vars["RO_Spring",ac(2:6)]                <- c(2,rep(3,4))          +101
BST.ctrl@obs.vars["RO_Autumn",ac(2:6)]                <- c(2,2,3,3,3)           +201
BST.ctrl@obs.vars["UKR_West", ac(2:9)]                <- c(2,3,3,3,3,4,4,4)     +301
BST.ctrl@obs.vars["BG_Spring",ac(2:9)]                <- c(2,2,3,3,3,3,4,4)     +401
BST.ctrl@obs.vars["BG_Autumn",ac(2:7)]                <- c(2,2,2,2,3,3)         +501
BST.ctrl@obs.vars["TR_Spring",ac(2:7)]                <- c(2,3,4,4,4,4)         +601
BST.ctrl@obs.vars["UKR_East", ac(2:9)]                <- c(2,2,3,3,4,5,6,6)     +701
BST.ctrl@obs.vars["TR_Autumn",ac(2:7)]                <- c(1,1,1,2,2,2)         +801
BST.ctrl                                              <- update(BST.ctrl)
```

Let's run again!

```{r, eval=T, echo=TRUE, message=FALSE, warning=FALSE}
output              <- capture.output(BST.sam             <- (FLSAM(BST,BST.tun,BST.ctrl,starting.values=BST.sam)))
save(BST,BST.tun,BST.ctrl,BST.sam,file="./DSML4.RData")
```

```{r, eval=T, echo=FALSE, message=FALSE, warning=FALSE}
obsvars <- obs.var(BST.sam)
print(xyplot(value+ubnd+lbnd ~ age | fleet,obsvars,
             scale=list(alternating=FALSE,y=list(relation="free")),as.table=TRUE,
             type="b",lwd=c(2,1,1),col=c("black","grey","grey"),pch=19,
             main="Observation variances",ylab="Observation variance",xlab="Age"))

```

Both the Ukrainian west and Turkish spring surveys have observation variances that are for their entire age range
close to or above 1. Seems that there is not much signal in this data.

Let's perform a Leave-one-out analyses to see what the actual influence of any of these surveys is.

Since there are some issues estimating one of the non-crucial parameters (random walk on surivors), we fix these parameters making use of the 'starting.values' option.

```{r, eval=T, echo=TRUE, message=FALSE, warning=FALSE}
init.sam            <- BST.sam
init.sam@params$value[which(init.sam@params$name=="logSdLogN")[2]] <- -5

#- Leave one out
BST.LOO             <- list()
for(i in 1:length(BST.tun))
  output <- capture.output(BST.LOO[[names(BST.tun)[i]]]             <- FLSAM(BST,BST.tun[-i],drop.from.control(BST.ctrl,fleets=names(BST.tun)[i]),
                                                                         starting.values=init.sam,map=list(logSdLogN=as.factor(c(-1.5,NA)))))
BST.LOO <- as(BST.LOO,"FLSAMs")

#- Leave one in
BST.LOI             <- list()
for(i in 1:length(BST.tun))
  output <- capture.output(BST.LOI[[names(BST.tun)[i]]]             <- FLSAM(BST,BST.tun[i],drop.from.control(BST.ctrl,fleets=names(BST.tun)[-i]),
                                                                         starting.values=init.sam,map=list(logSdLogN=as.factor(c(-1.5,NA)))))
BST.LOI <- as(BST.LOI,"FLSAMs")

plot(BST.LOO)
plot(BST.LOI)
save(BST,BST.tun,BST.ctrl,BST.LOO,BST.LOI,file="./DSML5.RData")

```{r, eval=T, echo=TRUE, message=FALSE, warning=FALSE}


Now that we've dropped each single survey one at a time, but also kept just one survey at a time, we make some diagnostic plots to see how much
influence each survey has

#- Let's calculate the Relative Percent Difference (RPD) for the leave one out runs
RPD <- array(NA,dim=c(length(1950:2018),3,length(BST.LOO)),dimnames=list(year=1950:2018,indicator=c("SSB","Fbar","Recruitment"),model=names(BST.LOO)))
for(iMod in 1:length(BST.LOO)){
  RPD[,"SSB",iMod]          <- (ssb(BST.LOO[[iMod]])$value - ssb(BST.sam)$value) / ssb(BST.sam)$value * 100
  RPD[,"Fbar",iMod]         <- (fbar(BST.LOO[[iMod]])$value - fbar(BST.sam)$value) / fbar(BST.sam)$value * 100
  RPD[,"Recruitment",iMod]  <- (rec(BST.LOO[[iMod]])$value - rec(BST.sam)$value) / rec(BST.sam)$value * 100
}
par(mfrow=c(1,2))
matplot(y=RPD[,"SSB",],x=1950:2018,type="b",xlab="Years",ylab="Relative percentage difference",lty=1,col=1:length(BST.LOO),pch=1:length(BST.LOO),main="SSB - LOO")
plot(1,1,col="white",bty="n",xaxt="n",yaxt="n",xlab="",ylab="")
legend("center",legend=names(BST.LOO),col=1:length(BST.LOO),pch=1:length(BST.LOO))

par(mfrow=c(1,2))
matplot(y=RPD[,"Fbar",],x=1950:2018,type="b",xlab="Years",ylab="Relative percentage difference",lty=1,col=1:length(BST.LOO),pch=1:length(BST.LOO),main="Fbar - LOO")
plot(1,1,col="white",bty="n",xaxt="n",yaxt="n",xlab="",ylab="")
legend("center",legend=names(BST.LOO),col=1:length(BST.LOO),pch=1:length(BST.LOO))

par(mfrow=c(1,2))
matplot(y=RPD[,"Recruitment",],x=1950:2018,type="b",xlab="Years",ylab="Relative percentage difference",lty=1,col=1:length(BST.LOO),pch=1:length(BST.LOO),main="Recruitment - LOO")
plot(1,1,col="white",bty="n",xaxt="n",yaxt="n",xlab="",ylab="")
legend("center",legend=names(BST.LOO),col=1:length(BST.LOO),pch=1:length(BST.LOO))

#- Let's calculate the Relative Percent Difference (RPD) for the leave one in runs
RPD <- array(NA,dim=c(length(1950:2018),3,length(BST.LOI)),dimnames=list(year=1950:2018,indicator=c("SSB","Fbar","Recruitment"),model=names(BST.LOI)))
for(iMod in 1:length(BST.LOI)){
  RPD[,"SSB",iMod]          <- (ssb(BST.LOI[[iMod]])$value - ssb(BST.sam)$value) / ssb(BST.sam)$value * 100
  RPD[,"Fbar",iMod]         <- (fbar(BST.LOI[[iMod]])$value - fbar(BST.sam)$value) / fbar(BST.sam)$value * 100
  RPD[,"Recruitment",iMod]  <- (rec(BST.LOI[[iMod]])$value - rec(BST.sam)$value) / rec(BST.sam)$value * 100
}
par(mfrow=c(1,2))
matplot(y=RPD[,"SSB",],x=1950:2018,type="b",xlab="Years",ylab="Relative percentage difference",lty=1,col=1:length(BST.LOI),pch=1:length(BST.LOI),main="SSB - LOI")
plot(1,1,col="white",bty="n",xaxt="n",yaxt="n",xlab="",ylab="")
legend("center",legend=names(BST.LOI),col=1:length(BST.LOI),pch=1:length(BST.LOI))

par(mfrow=c(1,2))
matplot(y=RPD[,"Fbar",],x=1950:2018,type="b",xlab="Years",ylab="Relative percentage difference",lty=1,col=1:length(BST.LOI),pch=1:length(BST.LOI),main="Fbar - LOI")
plot(1,1,col="white",bty="n",xaxt="n",yaxt="n",xlab="",ylab="")
legend("center",legend=names(BST.LOI),col=1:length(BST.LOI),pch=1:length(BST.LOI))

par(mfrow=c(1,2))
matplot(y=RPD[,"Recruitment",],x=1950:2018,type="b",xlab="Years",ylab="Relative percentage difference",lty=1,col=1:length(BST.LOI),pch=1:length(BST.LOI),main="Recruitment - LOI")
plot(1,1,col="white",bty="n",xaxt="n",yaxt="n",xlab="",ylab="")
legend("center",legend=names(BST.LOI),col=1:length(BST.LOI),pch=1:length(BST.LOI))

```

From the Leave-one-out runs, we learn that dropping the Romanian surveys hardly affects the estimation of SSB and Fbar. It does have a more profound effect in estimating recruitment.
Overall, there is very high variability in results between runs which indicates that the surveys are full of contrasting patterns and dropping just one can lead to a completely different result.
This is something we need to improve on.

It turns out that the Ukrainian surveys pull up SSB and pull down Fbar.

There is no room to drop any of the surveys at this stage, finding a model based rationale for doing so seems very difficult.

Finally, we want to check the plus-group. Given that there may be ageing problems at most likely older ages. We lower to age 5+. We need to trim some of the surveys as the same time!

```{r, eval=T, echo=TRUE, message=FALSE, warning=FALSE}
pgs       <- c(5:9)
pg.sams   <- list()
for(pg in sort(pgs)) {
  pg.stck   <- setPlusGroup(BST,pg)
  pg.stck@catch.wt[] <- pg.stck@stock.wt[] <- pg.stck@landings.wt
  pg.tun    <- BST.tun
  for(iTun in names(pg.tun)){
    if(range(pg.tun[[iTun]])["max"] >= pg)
      pg.tun[[iTun]] <- trim(pg.tun[[iTun]],age=2:(pg-1))
  }
  pg.ctrl <- drop.from.control(BST.ctrl,ages=((pg+1):BST.ctrl@range["max"]))
  pg.ctrl@catchabilities[-1,ac(pg)]                  <- -1
  pg.ctrl@obs.vars[-1,ac(pg)]                        <- -1
  pg.ctrl@states["catch unique",ac((pg-1):pg)]   <- 101
  pg.ctrl@obs.vars["catch unique",ac((pg-1):pg)] <- 101
  pg.ctrl@range[c("max","plusgroup")] <- pg
  if(pg < 8)
    pg.ctrl@range[c("max","plusgroup","maxfbar")]<- pg

  pg.ctrl <- update(pg.ctrl)

  #Perform assessment
  output <- capture.output(pg.sam <- FLSAM(pg.stck,pg.tun,pg.ctrl,starting.values=init.sam,map=list(logSdLogN=as.factor(c(-1.5,NA)))))

  #Store results
  pg.sam@name <- sprintf("%i+",pg)
  pg.sams[pg.sam@name] <- pg.sam
}
pg.sams[["10+"]] <- BST.sam
pg.sams <- as(pg.sams,"FLSAMs")
plot(pg.sams)

sel.pat <- merge(f(pg.sams),fbar(pg.sams),
                 by=c("year","name"),suffixes=c(".f",".fbar"))
sel.pat$sel <- sel.pat$value.f/sel.pat$value.fbar
sel.pat$age <- as.numeric(as.character(sel.pat$age))
print(xyplot(sel ~ age|sprintf("%i's",floor((year)/5)*5)*name,subset(sel.pat,year>2000),
             groups=year,type="l",as.table=TRUE,
             scale=list(alternating=FALSE),
             main="Selectivity of the Fishery by Pentad",xlab="Age",ylab="F/Fbar"))

save(BST,BST.tun,BST.ctrl,pg.sams,file="./DSML6.RData")
```

10+ group seems to behave markedly different from 6:9+ groups. The selection pattern goes from increasing to dome-shape and inflates biomass.
We run a retro to see if leaving in the extra few ages has some benefit, otherwise an 8 or 9+ may seem sensible.

```{r, eval=T, echo=TRUE, message=FALSE, warning=FALSE}
#- We cannot call the default retro function because it assumes surveys without year gaps
#BST.retro <- retro(BST,BST.tun,BST.ctrl,5)

BST.retro <- list()
BST.retro[[ac(2018)]] <- BST.sam
for(iRetroYr in 2017:2014){
  rt.stck <- window(BST,end=iRetroYr)
  rt.tun  <- BST.tun
  for(iTun in names(rt.tun)){
    if(range(rt.tun[[iTun]])["maxyear"] >= iRetroYr)
      rt.tun[[iTun]] <- rt.tun[[iTun]][,-which(dimnames(rt.tun[[iTun]]@index)$year %in% ((iRetroYr+1):range(rt.tun[[iTun]])["maxyear"]))]
  }
  rt.ctrl <- BST.ctrl
  rt.ctrl@range["maxyear"] <- iRetroYr
  output <- capture.output(rt.sam <- FLSAM(rt.stck,rt.tun,rt.ctrl,starting.values=init.sam,map=list(logSdLogN=as.factor(c(-1.5,NA)))))
  BST.retro[[ac(iRetroYr)]] <- rt.sam
}
BST.retro10plus <- as(BST.retro,"FLSAMs")

#- Now for the runs with lower plusgroup
pgs       <- c(8:9)
for(pg in sort(pgs)) {
  pg.stck   <- setPlusGroup(BST,pg)
  pg.stck@catch.wt[] <- pg.stck@stock.wt[] <- pg.stck@landings.wt
  pg.tun    <- BST.tun
  for(iTun in names(pg.tun)){
    if(range(pg.tun[[iTun]])["max"] >= pg)
      pg.tun[[iTun]] <- trim(pg.tun[[iTun]],age=range(pg.tun[[iTun]])["min"]:(pg-1))
  }
  pg.ctrl <- drop.from.control(BST.ctrl,ages=((pg+1):BST.ctrl@range["max"]))
  pg.ctrl@catchabilities[-1,ac(pg)]                  <- -1
  pg.ctrl@obs.vars[-1,ac(pg)]                        <- -1
  pg.ctrl@f.vars[1,ac(pg)]                           <- pg.ctrl@f.vars[1,ac(pg-1)]
  pg.ctrl@states["catch unique",ac((pg-1):pg)]   <- 101
  pg.ctrl@obs.vars["catch unique",ac((pg-1):pg)] <- 101
  pg.ctrl@range[c("max","plusgroup")] <- pg
  if(pg < 8)
    pg.ctrl@range[c("max","plusgroup","maxfbar")]<- pg
  pg.ctrl <- update(pg.ctrl)

  #Perform assessment
  output <- capture.output(pg.sam <- FLSAM(pg.stck,pg.tun,pg.ctrl,starting.values=init.sam,map=list(logSdLogN=as.factor(c(-1.5,NA)))))
  
#  pg9.sam <- pg.sam
#  pg9.tun <- pg.tun
#  pg9.ctrl <- pg.ctrl
#  pg9.stk <- pg.stck
#  save(pg9.stk,pg9.tun,pg9.ctrl,pg9.sam,file="D:/Repository/MSE_Adriatic/BST/pg9LongTimeseries.RData")
#
#  plot(FLSAMs(pg10=BST.sam,pg9=pg9.sam,pg8=pg8.sam))

  pg.init.sam <- pg.sam
  pg.init.sam@params$value[which(pg.init.sam@params$name=="logSdLogN")[2]] <- -5

  if(pg == 8){
    BST.retro <- list()
    BST.retro[[ac(2018)]] <- pg.sam
    for(iRetroYr in 2017:2014){
      rt.stck <- window(pg.stck,end=iRetroYr)
      rt.tun  <- pg.tun
      for(iTun in names(rt.tun)){
        if(range(rt.tun[[iTun]])["maxyear"] >= iRetroYr)
          rt.tun[[iTun]] <- rt.tun[[iTun]][,-which(dimnames(rt.tun[[iTun]]@index)$year %in% ((iRetroYr+1):range(rt.tun[[iTun]])["maxyear"]))]
      }
      rt.ctrl <- pg.ctrl
      rt.ctrl@residuals <- FALSE
      rt.ctrl@range["maxyear"] <- iRetroYr
      #output <- capture.output(
      rt.sam <- FLSAM(rt.stck,rt.tun,rt.ctrl,starting.values=pg.init.sam,map=list(logSdLogN=as.factor(c(-1.5,NA))))#)
      BST.retro[[ac(iRetroYr)]] <- rt.sam
    }
    BST.retro8plus <- as(BST.retro,"FLSAMs")
  }
  if(pg == 9){
    BST.retro <- list()
    BST.retro[[ac(2018)]] <- pg.sam
    for(iRetroYr in 2017:2014){
      rt.stck <- window(pg.stck,end=iRetroYr)
      rt.tun  <- pg.tun
      for(iTun in names(rt.tun)){
        if(range(rt.tun[[iTun]])["maxyear"] >= iRetroYr)
          rt.tun[[iTun]] <- rt.tun[[iTun]][,-which(dimnames(rt.tun[[iTun]]@index)$year %in% ((iRetroYr+1):range(rt.tun[[iTun]])["maxyear"]))]
      }
      rt.ctrl <- pg.ctrl
      rt.ctrl@residuals <- FALSE
      rt.ctrl@range["maxyear"] <- iRetroYr
      #output <- capture.output(
      rt.sam <- FLSAM(rt.stck,rt.tun,rt.ctrl,starting.values=pg.init.sam,map=list(logSdLogN=as.factor(c(-1.5,NA))))#)
      BST.retro[[ac(iRetroYr)]] <- rt.sam
    }
    BST.retro9plus <- as(BST.retro,"FLSAMs")
  }
}

storeMohnsRhos      <- matrix(NA,nrow=3,ncol=3,dimnames=list(plusgroup=paste0(8:10,"+"),type=c("ssb","fbar","rec")))
storeMohnsRhos[3,1] <- mean(mohns.rho(BST.retro10plus,ref.year=2018,type="ssb",span=4)[1:4,1])
storeMohnsRhos[2,1] <- mean(mohns.rho(BST.retro9plus,ref.year=2018,type="ssb",span=4)[1:4,1])
storeMohnsRhos[1,1] <- mean(mohns.rho(BST.retro8plus,ref.year=2018,type="ssb",span=4)[1:4,1])

storeMohnsRhos[3,2] <- mean(mohns.rho(BST.retro10plus,ref.year=2018,type="fbar",span=4)[1:4,1])
storeMohnsRhos[2,2] <- mean(mohns.rho(BST.retro9plus,ref.year=2018,type="fbar",span=4)[1:4,1])
storeMohnsRhos[1,2] <- mean(mohns.rho(BST.retro8plus,ref.year=2018,type="fbar",span=4)[1:4,1])

storeMohnsRhos[3,3] <- mean(mohns.rho(BST.retro10plus,ref.year=2018,type="rec",span=4)[1:4,1])
storeMohnsRhos[2,3] <- mean(mohns.rho(BST.retro9plus,ref.year=2018,type="rec",span=4)[1:4,1])
storeMohnsRhos[1,3] <- mean(mohns.rho(BST.retro8plus,ref.year=2018,type="rec",span=3)[1:4,1])

print(storeMohnsRhos)
plot(BST.retro10plus,main="10+")
plot(BST.retro9plus,main="9+")
plot(BST.retro8plus,main="8+")
save(BST,BST.ctrl,BST.tun,BST.retro10plus,BST.retro9plus,BST.retro8plus,file="DSML7.RData")
```

This concludes all data analyes, and we go for a plusgroup of age 9, as there seems to be a real issue with the 10+ in 2017 and 2018.
The cause for this is yet unknown and needs further exploration.

Now we move on to the next chapter which is fine-tuning of the model parameters.

#rmarkdown::render("BST_dataSelection.Rmd")

